<!doctype html><html lang=en><head><title>3DP-TP-00 | Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.650f674fc19266f4ea6c7b54bb6a4ffad10dfa641e3bea6de0a8472a99807590.css integrity="sha256-ZQ9nT8GSZvTqbHtUu2pP+tEN+mQeO+pt4KhHKpmAdZA="><link rel=icon type=image/png href=/images/site/favicon_hu994095d20d2022b1350b954a89d4b9d4_26338_42x0_resize_box_3.png><meta property="og:title" content="3DP-TP-00 | Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare"><meta property="og:description" content="Contrôle qualité d'objets 3D."><meta property="og:type" content="article"><meta property="og:url" content="https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/"><meta property="og:image" content="https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-03T08:06:25+06:00"><meta property="article:modified_time" content="2023-10-03T08:06:25+06:00"><meta name=description content="Contrôle qualité d'objets 3D."></head><body class="type-posts kind-page" data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg alt=Logo>
Noé GRACIA</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hud2c97a9f30beba9b284492c40f2d6757_11039_42x0_resize_q75_box.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ai/>Artificial Intelligence</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ai/computer-vision/>Computer Vision</a><ul class=active><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ai/computer-vision/3d_perception/_index.fr/>Perception 3D</a><ul class=active><li><a class=active href=/posts/ai/computer-vision/3d_perception/cc_segmentation/ title=Segmentation>Segmentation</a></li><li><a href=/posts/ai/computer-vision/3d_perception/monocular_localization/ title="Localisation mono.">Localisation mono.</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/ai/machine-learning/>Machine learning</a><ul><li><a href=/posts/ai/machine-learning/autoencoder/ title=Autoencoder>Autoencoder</a></li><li><a href=/posts/ai/machine-learning/segmentation/ title="Semantic Segmentation">Semantic Segmentation</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/>My life</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/about-me/>About me</a><ul><li><a href=/posts/my-life/about-me/my-setup/ title="My setup">My setup</a></li><li><a href=/posts/my-life/about-me/my-routine/ title="My routine">My routine</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/my-life/mountain/>Mountain</a><ul><li><a href=/posts/my-life/mountain/aneto/ title=Aneto>Aneto</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/noe_huee10e7eb3748b2b35cc484af7cf5c4fa_93893_120x120_fit_q75_box.jpeg alt="Author Image"><h5 class=author-name>Noé Gracia</h5><p>Tuesday, October 3, 2023</p></div><div class=title><h1>3DP-TP-00 | Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare</h1></div><div class=taxonomy-terms><ul style=padding-left:0><li class=rounded><a href=/tags/teaching/ class="btn, btn-sm">teaching</a></li><li class=rounded><a href=/tags/3d-perception/ class="btn, btn-sm">3D Perception</a></li></ul></div><div class=post-content id=post-content><h2 id=téléchargement-des-fichiers>Téléchargement des fichiers</h2><p>Les fichiers du TP peuvent être téléchargés sur votre page de cours dans Moodle, ou <em>via</em> <a href=files/files.zip>ce lien</a>.</p><h2 id=anchor-step-0>Définition des objectifs</h2><p>Ce TP vise à effectuer une segmentation 3D et un contrôle de forme sur des objets polyédriques, c&rsquo;est-à-dire à vérifier si ces objets sont corrects par rapport à un modèle géométrique de référence et/ou présentent des défauts (trous, résidus, etc.).</p><p><img src=images/objets_a_comparer.png alt="Objets à comparer" class=center><div style=margin-top:1rem></div></p><p>Pour cela, il faut au préalable construire ce modèle de référence à partir d&rsquo;une image RGB-D d&rsquo;un objet sans défaut. Ensuite, pour toute vue d&rsquo;un objet inconnu (dit &ldquo;de test&rdquo;), nous devons le segmenter à partir de l&rsquo;arrière-plan et le comparer avec le modèle de référence. Ce processus de vérification de la forme doit être indépendant du point de vue et,
exige donc l&rsquo;enregistrement de chaque nuage de points associé par rapport à celui de référence.</p><div class="alert alert-info"><strong><p>Nous proposons de décomposer cet objectif en trois étapes :</p><ul><li><a href=#anchor-step-1>étape 1</a> : <strong>extraire les points 3D</strong> des objets à comparer (à la fois objets de référence et de test) en supprimant tous les points de la scène n&rsquo;appartenant pas à l&rsquo;objet. Pour éviter un processus redondant, cette étape sera à réaliser seulement sur la scène de référence contenue dans <code>data01.xyz</code> ; cela a déjà été réalisé sur les objets à contrôler, et stocké dans les fichiers <code>data02_object.xyz</code> et <code>data03_object.xyz</code>.</li><li><a href=#anchor-step-2>étape 2</a> : <strong>enregistrer</strong> les points de chaque objet de test vers le modèle de référence afin de les comparer <em>i.e.</em> aligner leurs nuages de points 3D respectifs sur le repère de coordonnées de référence.</li><li><a href=#anchor-step-3>étape 3</a> : <strong>comparer</strong> les modèles de contrôle et de référence et conclure sur les potentiels défauts des modèles de contrôle.</li></ul></strong></div><h2 id=etape-1--extraction-du-modèle-3d-de-la-scène-de-référence><em><strong>Etape 1</strong></em> : extraction du modèle 3D de la scène de référence</h2><p>La première étape du TP consiste à extraire le nuage de points du modèle de référence à partir de la scène RGB-D acquise avec une Kinect :</p><p><img src=images/extraction_objet.png alt="Extraction du modèle de référence" class=center><div style=margin-top:1rem></div></p><p>Cette étape vise à calquer une surface plane sur le plan du sol, et à ne garder que la boite du centre en calculant la distance de chacun de ses points par rapport à ce plan et en y appliquant un seuil de filtrage.</p><p>Pour cela, ouvrez CloudCompare (le logiciel principal, pas le viewer) et importez les points de la scène <code>data01.xyz</code>. Sélectionnez le nuage en cliquant dessus dans le <em>workspace</em>.
A l&rsquo;aide de l&rsquo;outil de segmentation (<strong>Edit > Segment</strong>, ou bien directement le raccourci &ldquo;ciseaux&rdquo; dans la barre des raccourcis), divisez le nuage en trois sous-ensembles afin d&rsquo;en extraire le plan du sol et une zone grossière autour de la boite.
Le résultat obtenu est illustré par la figure suivante :</p><p><img src=images/extraction_modele.gif alt="Division de la scène en trois nuages" class=center><div style=margin-top:1rem></div></p><div class="alert alert-warning"><strong><p>Dans CloudCompare, pour travailler sur un nuage de points, il faut que la ligne lui correspondant soit sélectionnée dans le <em>workspace</em>. Vous savez si le nuage est sélectionné lorsqu&rsquo;une boite jaune s&rsquo;affiche autour.</p><p>Le fait de cocher la case ne sélectionne pas le nuage, elle le rend simplement visible/invisible dans l&rsquo;affichage.</p></strong></div><p>Créez une surface calquée sur le nuage du plan du sol à l&rsquo;aide de l&rsquo;outil <strong>Tools > Fit > Plane</strong>.
En sélectionnant le plan nouvellement créé et le nuage qui contient la boite, il est maintenant possible de calculer, pour chacun des points de ce nuage, sa distance au plan à l&rsquo;aide de l&rsquo;outil <strong>Tools > Distances > Cloud/Mesh Distance</strong> :</p><p><img src=images/fit_plane_compute_distance.png alt="Surface au plan et distance" class=center><div style=margin-top:1rem></div></p><p>L&rsquo;outil de distance ajoute un quatrième champ à chacun des points du nuage : la distance nouvellement calculée. En allant dans les propriétés du nuage, filtrez les points par rapport à ce champ scalaire pour ne garder que les points appartenant à la boite :</p><p><img src=images/filter_by_value.gif alt="Filtrage par la distance au plan" class=center><div style=margin-top:1rem></div></p><p>En cliquant sur <em>split</em>, deux nuages sont créés, correspondant aux deux côtés du filtrage :</p><p><img src=images/split.png alt="Boite extraite" class=center><div style=margin-top:1rem></div></p><p>Assurez-vous que le nuage nouvellement créé contient environ 10,000 points (le nombre de points est accessible dans le panneau des propriétés sur la gauche).</p><p>Sélectionnez seulement le nuage de la boite avant de l&rsquo;enregistrer au format ASCII Cloud sous le nom <code>data01_segmented.xyz</code> dans le dossier <code>data</code> du TP.</p><div class="alert alert-info"><strong>Par précaution, sauvegardez votre projet CloudCompare : pensez à <strong>sélectionner tous les nuages de points</strong>, et à sauvegarder le projet au format CloudCompare.</strong></div><h2 id=etape-2--enregistrement-des-points-3d><em><strong>Etape 2</strong></em> : enregistrement des points 3D</h2><p>Si vous avez ouvert les scènes complètes <code>data02.xyz</code> et <code>data03.xyz</code> dans CloudCompare, vous aurez remarqué que chaque scène a été prise d&rsquo;un point de vue légèrement différent, et que les objets eux-mêmes ont bougé :</p><p><img src=images/points_vue_diff.gif alt="Points de vue différents des scènes" class=center><div style=margin-top:1rem></div></p><p>Pour pouvoir comparer les modèles entre eux, on propose de les superposer et de calculer leur distance cumulée point à point. Plus cette distance est faible, plus les modèles se superposent et se ressemblent ; plus elle est grande, plus les modèles diffèrent.
L&rsquo;exemple suivant montre la superposition du modèle correct sur le modèle de référence précédemment extrait :</p><p><img src=images/plot_after_icp.png alt="Application d'ICP au modèle correct" class=center><div style=margin-top:1rem></div></p><p>Le fait de transformer les points d&rsquo;un modèle via une matrice de rotation/translation pour venir le superposer sur un autre nuage s&rsquo;appelle <em>l&rsquo;enregistrement des points</em>.
L&rsquo;algorithme <em><strong>Iterative Closest Point</strong></em> permet cet enregistrement, et nous proposons de l&rsquo;utiliser en Python.
Le code à modifier se situe uniquement dans <code>qualitycheck.py</code>, l&rsquo;objectif étant d&rsquo;appliquer ICP à la fois sur le modèle correct <code>data02_object.xyz</code>, et sur le modèle défectueux <code>data03_object.xyz</code>.</p><h3 id=chargement-des-modèles>Chargement des modèles</h3><p>La première partie du code charge les modèles <code>.xyz</code> extraits avec CloudCompare, stocke le modèle de référence dans la variable <code>ref</code> et le modèle à comparer dans la variable <code>data</code>.
Pour exécuter le code soit sur <code>data02_object</code>, soit sur <code>data03_object</code>, il suffit de commenter la ligne correspondante.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Load pre-processed model point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting MODEL object...&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data01_segmented.xyz&#39;</span>)[:,:<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load raw data point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting DATA02 object...&#34;</span>)
</span></span><span style=display:flex><span>data02_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data02_object.xyz&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load raw data point cloud</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Extracting DATA03 object...&#34;</span>)
</span></span><span style=display:flex><span>data03_object <span style=color:#f92672>=</span> datatools<span style=color:#f92672>.</span>load_XYZ_data_to_vec(<span style=color:#e6db74>&#39;data/data03_object.xyz&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ref <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> data02_object
</span></span><span style=display:flex><span><span style=color:#75715e># data = data03_object</span>
</span></span></code></pre></div><h3 id=appel-à-icp>Appel à ICP</h3><p>La deuxième partie du code consiste à coder l&rsquo;appel à la fonction <code>icp</code> de la librairie <code>icp</code>&mldr;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>##########################################################################</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Call ICP:</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   Here you have to call the icp function in icp library, get its return</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   variables and apply the transformation to the model in order to overlay</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#   it onto the reference model.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>4</span>)        <span style=color:#75715e># Transformation matrix returned by icp function</span>
</span></span><span style=display:flex><span>errors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>100</span>))  <span style=color:#75715e># Error value for each iteration of ICP</span>
</span></span><span style=display:flex><span>iterations <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>            <span style=color:#75715e># The total number of iterations applied by ICP</span>
</span></span><span style=display:flex><span>total_time<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>                <span style=color:#75715e># Total time of convergence of ICP</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------- YOUR TURN HERE -------- </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Draw results</span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>131</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Draw reference</span>
</span></span><span style=display:flex><span>datatools<span style=color:#f92672>.</span>draw_data(ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Reference&#39;</span>, ax<span style=color:#f92672>=</span>ax)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_subplot(<span style=color:#ae81ff>132</span>, projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Draw original data and reference</span>
</span></span><span style=display:flex><span>datatools<span style=color:#f92672>.</span>draw_data_and_ref(data, ref<span style=color:#f92672>=</span>ref, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>, ax<span style=color:#f92672>=</span>ax)
</span></span></code></pre></div><p>&mldr;et à stocker le retour de la fonction dans les variables <code>T</code>, <code>errors</code>, <code>iterations</code> et <code>total_time</code> comme défini par l&rsquo;en-tête de définition de la fonction dans le fichier <code>icp.py</code> :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>icp</span>(data, ref, init_pose<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, max_iterations<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, tolerance<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The Iterative Closest Point method: finds best-fit transform that maps points A on to points B
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Input:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A: Nxm numpy array of source mD points
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        B: Nxm numpy array of destination mD point
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        init_pose: (m+1)x(m+1) homogeneous transformation
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_iterations: exit algorithm after max_iterations
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tolerance: convergence criteria
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Output:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        T: final homogeneous transformation that maps A on to B
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        errors: Euclidean distances (errors) for max_iterations iterations in a (max_iterations+1) vector. distances[0] is the initial distance.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        i: number of iterations to converge
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        total_time : execution time
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span></code></pre></div><h3 id=transformation-du-modèle>Transformation du modèle</h3><p>La matrice <code>T</code> de transformation issue d&rsquo;ICP est la matrice de passage homogène permettant de calquer le modèle <code>data</code>, passé en paramètre de la fonction <code>icp</code>, sur le modèle <code>ref</code>.
Pour rappel, l&rsquo;application d&rsquo;une matrice homogène pour transformer un ensemble de points d&rsquo;un repère initial \(\mathcal{R_i}\) vers un repère final \(\mathcal{R_f}\) s&rsquo;effectue de la manière suivante :</p><p>$$P_f^{(4 \times N)} = T^{(4 \times 4)} . P_i^{(4 \times N)}$$</p><p>Dans le code, la troisième partie consiste donc à appliquer la matrice de transformation au modèle à comparer. Un exemple de l&rsquo;application d&rsquo;une matrice de rotation homogène à une autre matrice est donné ci-après :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># EXAMPLE of how to apply a homogeneous transformation to a set of points</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39; 
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (1) Make a homogeneous representation of the model to transform
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_model = np.ones((original_model.shape[0], 4))   ##### Construct a [N,4] matrix
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_model[:,0:3] = np.copy(original_model)          ##### Replace the X,Y,Z columns with the model points
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (2) Construct the R|t homogeneous transformation matrix / here a rotation of 36 degrees around x axis
</span></span></span><span style=display:flex><span><span style=color:#e6db74>theta = np.radians(36)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>c, s = np.cos(theta), np.sin(theta)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>homogeneous_matrix = np.array([[1, 0, 0, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, c, s, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, -s, c, 0],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                               [0, 0, 0, 1]])
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (3) Apply the transformation
</span></span></span><span style=display:flex><span><span style=color:#e6db74>transformed_model = np.dot(homogeneous_matrix, homogeneous_model.T).T
</span></span></span><span style=display:flex><span><span style=color:#e6db74># (4) Remove the homogeneous coordinate
</span></span></span><span style=display:flex><span><span style=color:#e6db74>transformed_model = np.delete(transformed_model, 3, 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;</span>
</span></span></code></pre></div><div class="alert alert-info"><strong><p>La variable <code>original</code> est un tableau de taille \(N \times 3\), \(N\) étant le nombre de points du modèle et 3 ses coordonnées \(X\), \(Y\) et \(Z\).</p><p>Il faut veiller à lui ajouter une coordonnée homogène et appliquer les transposées nécessaires pour que la multiplication de matrices fonctionne.
Aidez-vous de l&rsquo;exemple donné dans le code pour réaliser cette étape.</p></strong></div><p>Vous pouvez ensuite afficher le résultat en décommentant et complétant la ligne <code>datatools.draw_data...</code>.</p><h3 id=affichage-de-lerreur>Affichage de l&rsquo;erreur</h3><p>Décommentez et affichez l&rsquo;erreur dans la dernière partie du code, en changeant les &ldquo;&mldr;&rdquo; par les variables correspondantes :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Display error progress over time</span>
</span></span><span style=display:flex><span><span style=color:#75715e># **************** To be uncommented and completed ****************</span>
</span></span><span style=display:flex><span><span style=color:#75715e># fig1 = plt.figure(2, figsize=(20, 3))</span>
</span></span><span style=display:flex><span><span style=color:#75715e># it = np.arange(0, len(errors), 1)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.plot(it, ...)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.ylabel(&#39;Residual distance&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.xlabel(&#39;Iterations&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.title(&#39;Total elapsed time :&#39; + str(...) + &#39; s.&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># plt.show()</span>
</span></span></code></pre></div><h2 id=etape-3--comparaison-des-modèles><em><strong>Etape 3</strong></em> : comparaison des modèles</h2><p>Comparez l&rsquo;application d&rsquo;ICP sur les modèles <code>data02</code> et <code>data03</code>, remarquez l&rsquo;évolution de l&rsquo;erreur et les différences de valeurs.
Que représente cette erreur ? Que peut-on dire des deux modèles ? En vous appuyant sur les erreurs, quel seuil de décision pourriez-vous choisir pour déterminer si un modèle est défectueux ou non ?</p><h3 id=icp-dans-cloudcompare>ICP dans CloudCompare</h3><p>L&rsquo;algorithme ICP peut également être utilisé directement dans CloudCompare. Ouvrez-le et importez <code>data01_segmented.xyz</code>, <code>data02_object.xyz</code> et <code>data03_object.xyz</code>.</p><p>Sélectionnez par exemple les nuages de <code>data01_segmented</code> et <code>data02_object</code>, utilisez l&rsquo;outil <strong>Tools > Registration > Fine registration (ICP)</strong>. Assurez-vous que la référence est bien <code>data01</code> et appliquez ICP.
Son exécution vous renvoie la matrice de transformation calculée par l&rsquo;algorithme, et l&rsquo;applique à l&rsquo;objet.</p><p><img src=images/registration_cc.png alt="ICP dans CloudCompare" class=center><div style=margin-top:1rem></div></p><p>On peut ainsi, toujours en sélectionnant les deux nuages, calculer la distance entre les points avec <strong>Tools > Distance > Cloud/Cloud Distance</strong>. Assurez-vous que la référence est bien <code>data01</code> et cliquez sur OK/Compute/OK.
Sélectionnez <code>data02_object</code> et affichez l&rsquo;histogramme de ses distances au nuage de référence <em>via</em> <strong>Edit > Scalar fields > Show histogram</strong>.</p><p><img src=images/histogram_dists.png alt="Histogramme des distances point à point de data02 vs. data01" class=center><div style=margin-top:1rem></div></p><p>Faites la même chose avec <code>data03_object</code> et comparez les histogrammes. Comment les interprétez-vous ? Comment pouvez-vous les comparer ?</p><p>TP crée par notre proffesseur <a href=https://clairelabitbonis.github.io/ target=_blank rel=noopener>Claire Labit-Bonis</a>.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&text=3DP-TP-00%20%7c%20Segmentation%20de%20nuages%20de%20points%203D%20par%20capteur%20%c3%a0%20lumi%c3%a8re%20structur%c3%a9e%20RGB-D%20avec%20CloudCompare&via=No%c3%a9%20GRACIA" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&title=3DP-TP-00%20%7c%20Segmentation%20de%20nuages%20de%20points%203D%20par%20capteur%20%c3%a0%20lumi%c3%a8re%20structur%c3%a9e%20RGB-D%20avec%20CloudCompare" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f&title=3DP-TP-00%20%7c%20Segmentation%20de%20nuages%20de%20points%203D%20par%20capteur%20%c3%a0%20lumi%c3%a8re%20structur%c3%a9e%20RGB-D%20avec%20CloudCompare" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=3DP-TP-00%20%7c%20Segmentation%20de%20nuages%20de%20points%203D%20par%20capteur%20%c3%a0%20lumi%c3%a8re%20structur%c3%a9e%20RGB-D%20avec%20CloudCompare https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn btn-sm email-btn" href="mailto:?subject=3DP-TP-00%20%7c%20Segmentation%20de%20nuages%20de%20points%203D%20par%20capteur%20%c3%a0%20lumi%c3%a8re%20structur%c3%a9e%20RGB-D%20avec%20CloudCompare&body=https%3a%2f%2fnoegracia.github.io%2fposts%2fai%2fcomputer-vision%2f3d_perception%2fcc_segmentation%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/noegracia/noegracia.github.io/edit/main/content/posts/ai/computer-vision/3d_perception/cc_segmentation/index.fr.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/ai/computer-vision/3d_perception/monocular_localization/ title="3DP-TP-01 | Localisation monoculaire par PnL itérative" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>3DP-TP-01 | Localisation monoculaire par PnL itérative</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#téléchargement-des-fichiers>Téléchargement des fichiers</a></li><li><a href=#anchor-step-0>Définition des objectifs</a></li><li><a href=#etape-1--extraction-du-modèle-3d-de-la-scène-de-référence><em><strong>Etape 1</strong></em> : extraction du modèle 3D de la scène de référence</a></li><li><a href=#etape-2--enregistrement-des-points-3d><em><strong>Etape 2</strong></em> : enregistrement des points 3D</a><ul><li><a href=#chargement-des-modèles>Chargement des modèles</a></li><li><a href=#appel-à-icp>Appel à ICP</a></li><li><a href=#transformation-du-modèle>Transformation du modèle</a></li><li><a href=#affichage-de-lerreur>Affichage de l&rsquo;erreur</a></li></ul></li><li><a href=#etape-3--comparaison-des-modèles><em><strong>Etape 3</strong></em> : comparaison des modèles</a><ul><li><a href=#icp-dans-cloudcompare>ICP dans CloudCompare</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://noegracia.github.io#education>Education</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:noegraciagirona@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>noegraciagirona@gmail.com</span></a></li><li><a href=https://www.linkedin.com/in/noegracia target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Noé Gracia</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2023 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.710b031badd7b32dd74ed8c97e636fdcdbd226e1fbd0ac60dbfd971c036a640a.js integrity="sha256-cQsDG63Xsy3XTtjJfmNv3NvSJuH70Kxg2/2XHANqZAo=" defer></script></body></html>