[{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/introduction/","summary":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","tags":null,"title":"Go পরিচিতি"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/_index.bn/","summary":"","tags":null,"title":"Go বেসিক"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/advanced/_index.bn/","summary":"","tags":null,"title":"অ্যাডভান্সড"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/types/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"বেসিক টাইপ সমূহ"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/basic/flow-control/","summary":"Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/advanced/files/","summary":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","tags":null,"title":"ফাইল ম্যানিপুলেশন"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/bash/basic/","summary":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","tags":null,"title":"ব্যাশ ভেরিয়েবল"},{"categories":["Life"],"contents":"Ascending Aneto: Conquering the Pyrenees\u0026rsquo;s Pinnacle The adventure began with anticipation as we gathered our gear, crampons and ice axes, essential for the challenge ahead. From the starting elevation of 1700 meters, we embarked on our ascent to 2140 meters, finding sanctuary at the Renclusa Refuge. Dinner was our prelude to rest, yet sleep proved elusive. At 9 PM, I nestled into my bunk, but the altitude, the cold, and a symphony of nocturnal noises (someone snoring) allowed for only two hours of light slumber.\nAs I wake up at 6 AM, I had breakfast and then started our climbing non stop. For us novices, the path was not without its confusions; a few missteps cost us time but we could find our path by following experienced people that also were climbing the mountain. We trekked tirelessly until 1 PM, when we reached the 3000-meter mark and attempted to refuel. Altitude induced a sense of anorexia, making each bite of our sandwiches a laborious task. I managed only two-thirds of mine, despite usually having a robust appetite.\nIt was here that one of our friends, suffering from fatigue and having mountain sickness, chose to stay behind. The rest of us, determined, pressed on towards Aneto\u0026rsquo;s zenith. Seven hours of climbing rewarded us with a panoramic spectacle, we had beauty in every direction. The ascent was arduous, but the inspiring views at every altitude, from the valley\u0026rsquo;s charm at the base to the alpenglow of sunset painting the sky during our descent, were the true treasures of the journey.\nOur descent back to the car at 1700 meters marked the culmination of a 12-hour odyssey, not counting the previous night\u0026rsquo;s efforts. It was a 12-hour journey of traversing snowfields, dragging our legs through the snow, climbing, having lack of oxygen, falling in crevasses (hopefully not very deep ones). This grueling 15-hour journey was probably one of the most demanding physical feats I have ever undertaken.\nPhoto at the top of the mountain :P\n","date":"February 5, 2024","hero":"/posts/my-life/mountain/aneto/subida.jpg","permalink":"https://noegracia.github.io/posts/my-life/mountain/aneto/","summary":"Ascending Aneto: Conquering the Pyrenees\u0026rsquo;s Pinnacle The adventure began with anticipation as we gathered our gear, crampons and ice axes, essential for the challenge ahead. From the starting elevation of 1700 meters, we embarked on our ascent to 2140 meters, finding sanctuary at the Renclusa Refuge. Dinner was our prelude to rest, yet sleep proved elusive. At 9 PM, I nestled into my bunk, but the altitude, the cold, and a symphony of nocturnal noises (someone snoring) allowed for only two hours of light slumber.","tags":["life","routine"],"title":"Aneto"},{"categories":null,"contents":"Téléchargement des fichiers Les fichiers du TP peuvent être téléchargés sur votre page de cours dans Moodle, ou via ce lien.\nDéfinition des objectifs Ce TP vise à effectuer une segmentation 3D et un contrôle de forme sur des objets polyédriques, c\u0026rsquo;est-à-dire à vérifier si ces objets sont corrects par rapport à un modèle géométrique de référence et/ou présentent des défauts (trous, résidus, etc.).\nPour cela, il faut au préalable construire ce modèle de référence à partir d\u0026rsquo;une image RGB-D d\u0026rsquo;un objet sans défaut. Ensuite, pour toute vue d\u0026rsquo;un objet inconnu (dit \u0026ldquo;de test\u0026rdquo;), nous devons le segmenter à partir de l\u0026rsquo;arrière-plan et le comparer avec le modèle de référence. Ce processus de vérification de la forme doit être indépendant du point de vue et, exige donc l\u0026rsquo;enregistrement de chaque nuage de points associé par rapport à celui de référence.\nNous proposons de décomposer cet objectif en trois étapes :\nétape 1 : extraire les points 3D des objets à comparer (à la fois objets de référence et de test) en supprimant tous les points de la scène n\u0026rsquo;appartenant pas à l\u0026rsquo;objet. Pour éviter un processus redondant, cette étape sera à réaliser seulement sur la scène de référence contenue dans data01.xyz ; cela a déjà été réalisé sur les objets à contrôler, et stocké dans les fichiers data02_object.xyz et data03_object.xyz. étape 2 : enregistrer les points de chaque objet de test vers le modèle de référence afin de les comparer i.e. aligner leurs nuages de points 3D respectifs sur le repère de coordonnées de référence. étape 3 : comparer les modèles de contrôle et de référence et conclure sur les potentiels défauts des modèles de contrôle. Etape 1 : extraction du modèle 3D de la scène de référence La première étape du TP consiste à extraire le nuage de points du modèle de référence à partir de la scène RGB-D acquise avec une Kinect :\nCette étape vise à calquer une surface plane sur le plan du sol, et à ne garder que la boite du centre en calculant la distance de chacun de ses points par rapport à ce plan et en y appliquant un seuil de filtrage.\nPour cela, ouvrez CloudCompare (le logiciel principal, pas le viewer) et importez les points de la scène data01.xyz. Sélectionnez le nuage en cliquant dessus dans le workspace. A l\u0026rsquo;aide de l\u0026rsquo;outil de segmentation (Edit \u0026gt; Segment, ou bien directement le raccourci \u0026ldquo;ciseaux\u0026rdquo; dans la barre des raccourcis), divisez le nuage en trois sous-ensembles afin d\u0026rsquo;en extraire le plan du sol et une zone grossière autour de la boite. Le résultat obtenu est illustré par la figure suivante :\nDans CloudCompare, pour travailler sur un nuage de points, il faut que la ligne lui correspondant soit sélectionnée dans le workspace. Vous savez si le nuage est sélectionné lorsqu\u0026rsquo;une boite jaune s\u0026rsquo;affiche autour.\nLe fait de cocher la case ne sélectionne pas le nuage, elle le rend simplement visible/invisible dans l\u0026rsquo;affichage.\nCréez une surface calquée sur le nuage du plan du sol à l\u0026rsquo;aide de l\u0026rsquo;outil Tools \u0026gt; Fit \u0026gt; Plane. En sélectionnant le plan nouvellement créé et le nuage qui contient la boite, il est maintenant possible de calculer, pour chacun des points de ce nuage, sa distance au plan à l\u0026rsquo;aide de l\u0026rsquo;outil Tools \u0026gt; Distances \u0026gt; Cloud/Mesh Distance :\nL\u0026rsquo;outil de distance ajoute un quatrième champ à chacun des points du nuage : la distance nouvellement calculée. En allant dans les propriétés du nuage, filtrez les points par rapport à ce champ scalaire pour ne garder que les points appartenant à la boite :\nEn cliquant sur split, deux nuages sont créés, correspondant aux deux côtés du filtrage :\nAssurez-vous que le nuage nouvellement créé contient environ 10,000 points (le nombre de points est accessible dans le panneau des propriétés sur la gauche).\nSélectionnez seulement le nuage de la boite avant de l\u0026rsquo;enregistrer au format ASCII Cloud sous le nom data01_segmented.xyz dans le dossier data du TP.\nPar précaution, sauvegardez votre projet CloudCompare : pensez à sélectionner tous les nuages de points, et à sauvegarder le projet au format CloudCompare. Etape 2 : enregistrement des points 3D Si vous avez ouvert les scènes complètes data02.xyz et data03.xyz dans CloudCompare, vous aurez remarqué que chaque scène a été prise d\u0026rsquo;un point de vue légèrement différent, et que les objets eux-mêmes ont bougé :\nPour pouvoir comparer les modèles entre eux, on propose de les superposer et de calculer leur distance cumulée point à point. Plus cette distance est faible, plus les modèles se superposent et se ressemblent ; plus elle est grande, plus les modèles diffèrent. L\u0026rsquo;exemple suivant montre la superposition du modèle correct sur le modèle de référence précédemment extrait :\nLe fait de transformer les points d\u0026rsquo;un modèle via une matrice de rotation/translation pour venir le superposer sur un autre nuage s\u0026rsquo;appelle l\u0026rsquo;enregistrement des points. L\u0026rsquo;algorithme Iterative Closest Point permet cet enregistrement, et nous proposons de l\u0026rsquo;utiliser en Python. Le code à modifier se situe uniquement dans qualitycheck.py, l\u0026rsquo;objectif étant d\u0026rsquo;appliquer ICP à la fois sur le modèle correct data02_object.xyz, et sur le modèle défectueux data03_object.xyz.\nChargement des modèles La première partie du code charge les modèles .xyz extraits avec CloudCompare, stocke le modèle de référence dans la variable ref et le modèle à comparer dans la variable data. Pour exécuter le code soit sur data02_object, soit sur data03_object, il suffit de commenter la ligne correspondante.\n# Load pre-processed model point cloud print(\u0026#34;Extracting MODEL object...\u0026#34;) model = datatools.load_XYZ_data_to_vec(\u0026#39;data/data01_segmented.xyz\u0026#39;)[:,:3] # Load raw data point cloud print(\u0026#34;Extracting DATA02 object...\u0026#34;) data02_object = datatools.load_XYZ_data_to_vec(\u0026#39;data/data02_object.xyz\u0026#39;) # Load raw data point cloud print(\u0026#34;Extracting DATA03 object...\u0026#34;) data03_object = datatools.load_XYZ_data_to_vec(\u0026#39;data/data03_object.xyz\u0026#39;) ref = model data = data02_object # data = data03_object Appel à ICP La deuxième partie du code consiste à coder l\u0026rsquo;appel à la fonction icp de la librairie icp\u0026hellip;\n########################################################################## # Call ICP: # Here you have to call the icp function in icp library, get its return # variables and apply the transformation to the model in order to overlay # it onto the reference model. matrix = np.eye(4,4) # Transformation matrix returned by icp function errors = np.zeros((1,100)) # Error value for each iteration of ICP iterations = 100 # The total number of iterations applied by ICP total_time=0 # Total time of convergence of ICP # ------- YOUR TURN HERE -------- # Draw results fig = plt.figure(1, figsize=(20, 5)) ax = fig.add_subplot(131, projection=\u0026#39;3d\u0026#39;) # Draw reference datatools.draw_data(ref, title=\u0026#39;Reference\u0026#39;, ax=ax) ax = fig.add_subplot(132, projection=\u0026#39;3d\u0026#39;) # Draw original data and reference datatools.draw_data_and_ref(data, ref=ref, title=\u0026#39;Raw data\u0026#39;, ax=ax) \u0026hellip;et à stocker le retour de la fonction dans les variables T, errors, iterations et total_time comme défini par l\u0026rsquo;en-tête de définition de la fonction dans le fichier icp.py :\ndef icp(data, ref, init_pose=None, max_iterations=20, tolerance=0.001): \u0026#39;\u0026#39;\u0026#39; The Iterative Closest Point method: finds best-fit transform that maps points A on to points B Input: A: Nxm numpy array of source mD points B: Nxm numpy array of destination mD point init_pose: (m+1)x(m+1) homogeneous transformation max_iterations: exit algorithm after max_iterations tolerance: convergence criteria Output: T: final homogeneous transformation that maps A on to B errors: Euclidean distances (errors) for max_iterations iterations in a (max_iterations+1) vector. distances[0] is the initial distance. i: number of iterations to converge total_time : execution time \u0026#39;\u0026#39;\u0026#39; Transformation du modèle La matrice T de transformation issue d\u0026rsquo;ICP est la matrice de passage homogène permettant de calquer le modèle data, passé en paramètre de la fonction icp, sur le modèle ref. Pour rappel, l\u0026rsquo;application d\u0026rsquo;une matrice homogène pour transformer un ensemble de points d\u0026rsquo;un repère initial \\(\\mathcal{R_i}\\) vers un repère final \\(\\mathcal{R_f}\\) s\u0026rsquo;effectue de la manière suivante :\n$$P_f^{(4 \\times N)} = T^{(4 \\times 4)} . P_i^{(4 \\times N)}$$\nDans le code, la troisième partie consiste donc à appliquer la matrice de transformation au modèle à comparer. Un exemple de l\u0026rsquo;application d\u0026rsquo;une matrice de rotation homogène à une autre matrice est donné ci-après :\n# EXAMPLE of how to apply a homogeneous transformation to a set of points \u0026#39;\u0026#39;\u0026#39; # (1) Make a homogeneous representation of the model to transform homogeneous_model = np.ones((original_model.shape[0], 4)) ##### Construct a [N,4] matrix homogeneous_model[:,0:3] = np.copy(original_model) ##### Replace the X,Y,Z columns with the model points # (2) Construct the R|t homogeneous transformation matrix / here a rotation of 36 degrees around x axis theta = np.radians(36) c, s = np.cos(theta), np.sin(theta) homogeneous_matrix = np.array([[1, 0, 0, 0], [0, c, s, 0], [0, -s, c, 0], [0, 0, 0, 1]]) # (3) Apply the transformation transformed_model = np.dot(homogeneous_matrix, homogeneous_model.T).T # (4) Remove the homogeneous coordinate transformed_model = np.delete(transformed_model, 3, 1) \u0026#39;\u0026#39;\u0026#39; La variable original est un tableau de taille \\(N \\times 3\\), \\(N\\) étant le nombre de points du modèle et 3 ses coordonnées \\(X\\), \\(Y\\) et \\(Z\\).\nIl faut veiller à lui ajouter une coordonnée homogène et appliquer les transposées nécessaires pour que la multiplication de matrices fonctionne. Aidez-vous de l\u0026rsquo;exemple donné dans le code pour réaliser cette étape.\nVous pouvez ensuite afficher le résultat en décommentant et complétant la ligne datatools.draw_data....\nAffichage de l\u0026rsquo;erreur Décommentez et affichez l\u0026rsquo;erreur dans la dernière partie du code, en changeant les \u0026ldquo;\u0026hellip;\u0026rdquo; par les variables correspondantes :\n# Display error progress over time # **************** To be uncommented and completed **************** # fig1 = plt.figure(2, figsize=(20, 3)) # it = np.arange(0, len(errors), 1) # plt.plot(it, ...) # plt.ylabel(\u0026#39;Residual distance\u0026#39;) # plt.xlabel(\u0026#39;Iterations\u0026#39;) # plt.title(\u0026#39;Total elapsed time :\u0026#39; + str(...) + \u0026#39; s.\u0026#39;) # plt.show() Etape 3 : comparaison des modèles Comparez l\u0026rsquo;application d\u0026rsquo;ICP sur les modèles data02 et data03, remarquez l\u0026rsquo;évolution de l\u0026rsquo;erreur et les différences de valeurs. Que représente cette erreur ? Que peut-on dire des deux modèles ? En vous appuyant sur les erreurs, quel seuil de décision pourriez-vous choisir pour déterminer si un modèle est défectueux ou non ?\nICP dans CloudCompare L\u0026rsquo;algorithme ICP peut également être utilisé directement dans CloudCompare. Ouvrez-le et importez data01_segmented.xyz, data02_object.xyz et data03_object.xyz.\nSélectionnez par exemple les nuages de data01_segmented et data02_object, utilisez l\u0026rsquo;outil Tools \u0026gt; Registration \u0026gt; Fine registration (ICP). Assurez-vous que la référence est bien data01 et appliquez ICP. Son exécution vous renvoie la matrice de transformation calculée par l\u0026rsquo;algorithme, et l\u0026rsquo;applique à l\u0026rsquo;objet.\nOn peut ainsi, toujours en sélectionnant les deux nuages, calculer la distance entre les points avec Tools \u0026gt; Distance \u0026gt; Cloud/Cloud Distance. Assurez-vous que la référence est bien data01 et cliquez sur OK/Compute/OK. Sélectionnez data02_object et affichez l\u0026rsquo;histogramme de ses distances au nuage de référence via Edit \u0026gt; Scalar fields \u0026gt; Show histogram.\nFaites la même chose avec data03_object et comparez les histogrammes. Comment les interprétez-vous ? Comment pouvez-vous les comparer ?\nTP crée par notre proffesseur Claire Labit-Bonis.\n","date":"October 3, 2023","hero":"/posts/ai/computer-vision/3d_perception/cc_segmentation/featured.png","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/cc_segmentation/","summary":"Contrôle qualité d\u0026rsquo;objets 3D.","tags":["teaching","3D Perception"],"title":"3DP-TP-00 | Segmentation de nuages de points 3D par capteur à lumière structurée RGB-D avec CloudCompare"},{"categories":null,"contents":"","date":"October 3, 2023","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/_index.fr/","summary":"","tags":null,"title":"Perception 3D"},{"categories":["Life"],"contents":"My Daily Routine Sports and Physical Activities Every day, I make sure to prioritize my physical well-being. My routine includes approximately 30 minutes of biking, which can vary depending on whether I\u0026rsquo;m working remotely or other factors. According to the World Health Organisation adults aged between 18-64 years should do at least 150–300 minutes of moderate-intensity aerobic physical activity, so I always like to do some cardio, moreover it makes me feel good. Apart from cardio I also like to do muscle strengthening. I usually do push pull legs workouts twice a week, whether it\u0026rsquo;s at the gym or at street workout parks. However, I also like doing diverse physical activities so I usually skip my workout days to do sports like tennis, volleyball, football, and even dance. My workouts are enough intense to progress but not too intense to be able to stick to my routine without being fatigued.\nMeal Preps Nutrition is a crucial part of maintaining a balanced lifestyle. I\u0026rsquo;m a dedicated meal prepper, ensuring that I have a nutritious meal ready for the next day to avoid long meal breaks. My meal prep is pretty simple. It always includes 3 important components: protein sources like chicken, salmon, or beef, coupled with carbohydrates like rice or pasta, and vegetables. On weekends, I allow myself to be more flexible, enjoying restaurant meals. I eat a lot of fruits, my favorites being bananas, kiwis and tangerines in season.\nSleep A good night\u0026rsquo;s sleep is non-negotiable in my routine. I aim for a solid 7-8 hours of rest every night. To ensure a peaceful night\u0026rsquo;s sleep, I have a calming bedtime routine. I brush my teeths, do my skincare and then I like to wind down with a good book; currently, I\u0026rsquo;m immersed in Brandon Sanderson\u0026rsquo;s \u0026ldquo;The Final Empire\u0026rdquo;. I make sure to close all windows and shut off every light, creating the perfect sleep environment.\n","date":"August 7, 2023","hero":"/posts/my-life/about-me/my-routine/velo.jpg","permalink":"https://noegracia.github.io/posts/my-life/about-me/my-routine/","summary":"My Daily Routine Sports and Physical Activities Every day, I make sure to prioritize my physical well-being. My routine includes approximately 30 minutes of biking, which can vary depending on whether I\u0026rsquo;m working remotely or other factors. According to the World Health Organisation adults aged between 18-64 years should do at least 150–300 minutes of moderate-intensity aerobic physical activity, so I always like to do some cardio, moreover it makes me feel good.","tags":["life","routine"],"title":"My routine"},{"categories":["Life"],"contents":"Setup My coding setup consists of a dual-monitor arrangement and a high-performance PC featuring a GeForce 3080, an i7-12700KF, 32GB RAM 2400MHz, and a 2TB SSD. This configuration allows me to efficiently handle large datasets and run resource-intensive algorithms, enhancing my productivity.\n","date":"August 6, 2023","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/posts/my-life/about-me/my-setup/","summary":"Setup My coding setup consists of a dual-monitor arrangement and a high-performance PC featuring a GeForce 3080, an i7-12700KF, 32GB RAM 2400MHz, and a 2TB SSD. This configuration allows me to efficiently handle large datasets and run resource-intensive algorithms, enhancing my productivity.","tags":["pc"],"title":"My setup"},{"categories":null,"contents":" Cet exercice consiste à remplir des fonctions ou des morceaux vides dans le fichier localisation.py :\nétape 1 : perspective_projection et transform_and_draw_model, étape 2 : calculate_normal_vector et calculate_error étape 3 : partial_derivatives, étape 4 : transformation vers un deuxième point de vue. Chaque fonction et le rôle qu\u0026rsquo;elle remplit sont décrits dans cet article.\nNe vous jetez pas dans le codage des fonctions dès la partie \u0026ldquo;définition des objectifs\u0026rdquo;, qui n\u0026rsquo;est qu\u0026rsquo;une explication théorique globale pour présenter le sujet. Chaque étape et les fonctions qui qui sont propres sont décrites en détail dans les parties correspondantes : étape 1, étape 2, étape 3, étape 4. Téléchargement des fichiers Les fichiers du TP peuvent être téléchargés sur votre page de cours dans Moodle, ou via ce lien.\nDéfinition des objectifs L\u0026rsquo;objectif de cet exercice est de trouver la transformation optimale à appliquer à un modèle 3D exprimé en centimètres dans un repère objet \\(\\mathcal{R_o} (X,Y,Z)\\) (parfois appelé repère monde \\(\\mathcal{R_w}\\)) afin de le dessiner dans une image 2D exprimée en pixels dans un repère image \\(\\mathcal{R_i} (u,v)\\). Le modèle 3D a été téléchargé sur free3d et modifié dans Blender pour les besoins de l\u0026rsquo;exercice. Ses points et arêtes ont respectivement été exportés via un script Python dans les fichiers pikachu.xyz et pikachu.edges.\npikachu.xyz contains 134 lines and 3 columns, corresponding to the 134 model points and their three \\(x, y, z\\) coordinates.\npikachu.edges contient 246 lignes et 2 colonnes, correspondant aux 246 arêtes du modèle et aux 2 indices de leurs extrémités (la première arête est définie par son premier point à l\u0026rsquo;indice 2 et par son deuxième point à l\u0026rsquo;indice 0 dans pikachu.xyz).\nPar \u0026ldquo;transformation\u0026rdquo;, on entend une rotation et une translation de la scène sur les axes \\(x\\), \\(y\\) et \\(z\\) des points dans \\(\\mathcal{R_o}\\).\nDans notre cas, on veut augmenter la réalité capturée par la caméra et positionner le modèle de Pikachu sur le cube bleu ciel de l\u0026rsquo;image ci-dessous, en le faisant parfaitement matcher avec le cube virtuel sur lequel il repose.\nParamètres intrinsèques. Cliquez pour étendre On utilise le modèle de caméra sténopé qui permet de réaliser cette opération en deux transformations successives :\\(\\mathcal{R_o} \\rightarrow \\mathcal{R_c}\\) puis \\(\\mathcal{R_c} \\rightarrow \\mathcal{R_i}\\). En plus des repères \\(\\mathcal{R_o}\\) et \\(\\mathcal{R_i}\\), il faut donc aussi considérer le repère caméra \\(\\mathcal{R_c}\\).\nLa ressource Modélisation et calibrage d\u0026rsquo;une caméra décrit en détail le modèle sténopé et peut aider à la compréhension de l\u0026rsquo;exercice.\nWikipédia définit le modèle sténopé comme décrivant la relation mathématique entre les coordonnées d\u0026rsquo;un point dans un espace en trois dimensions et sa projection dans le plan image d\u0026rsquo;une caméra sténopé i.e. une caméra dont l\u0026rsquo;ouverture est décrite comme un point et qui n\u0026rsquo;utilise pas de lentille pour focaliser la lumière.\nLe passage entre les repères \\(\\mathcal{R_c}\\) anetd \\(\\mathcal{R_i}\\) se fait à l\u0026rsquo;aide des paramètres intrinsèques de la caméra. Ces coefficients \\((\\alpha_u, \\alpha_v, u_0, v_0)\\) sont ensuite stockés dans une matrice de passage homogène \\(K_{i \\leftarrow c}\\) de sorte que l\u0026rsquo;on peut décrire la relation \\(p_i = K_{i \\leftarrow c}.P_c\\) comme étant :\n$$ \\begin{bmatrix} u\\\\v\\\\1 \\end{bmatrix}_{\\mathcal{R}_i} = s. \\begin{bmatrix} \\alpha_u \u0026amp; 0 \u0026amp; u_0\\\\ 0 \u0026amp; \\alpha_v \u0026amp; v_0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} . \\begin{bmatrix} X\\\\ Y\\\\ Z \\end{bmatrix}_{\\mathcal{R}_c} $$\navec :\n\\(p_i\\) (à gauche) le point exprimé en pixels dans le repère 2D image \\(\\mathcal{R_i}\\), \\(P_c\\) (à droite) le point exprimé en centimètres dans le repère 3D caméra \\(\\mathcal{R_c}\\), \\(s = \\frac{1}{Z}\\), \\(\\alpha_u = k_x f\\), \\(\\alpha_v = k_y f\\) : \\(k_x = k_y\\) le nombre de pixels par millimètre du capteur dans les directions \\(x\\) et \\(y\\) \u0026ndash; l\u0026rsquo;égalité n\u0026rsquo;étant vraie que si les pixels sont carrés, \\(f\\) la distance focale. \\(u_0\\) et \\(v_0\\) les centres de l\u0026rsquo;image en pixels dans \\(\\mathcal{R}_i\\). Dans notre cas, l\u0026rsquo;image a été capturée par un Canon EOS 700D dont la taille du capteur est de \\(22.3\\times14.9 mm\\). La taille de l\u0026rsquo;image étant de \\(720\\times480 px\\) et la distance focale de \\(18mm\\), on en déduit les paramètres \\(\\alpha_u = 581.1659\\), \\(\\alpha_v = 579.8657\\), \\(u_0 = 360\\) et \\(v_0 = 240\\) qui sont stockés dans le fichier calibration_parameters.txt.\nParamètres extrinsèques. Cliquez pour étendre Pour atteindre notre objectif de départ, c\u0026rsquo;est-à-dire afficher dans l\u0026rsquo;image en pixels notre modèle 3D virtuel, nous devons estimer les coefficients de la transformation \\(\\mathcal{R_o}\\rightarrow\\mathcal{R_c}\\) permettant d\u0026rsquo;établir la relation \\(P_c=M_{c\\leftarrow o}.P_o\\), avec :\n\\(M_{c\\leftarrow o}=\\big[ R_{\\alpha\\beta\\gamma} | T \\big]\\) la matrice de transformation homogène composée des angles d\u0026rsquo;Euler et de la translation selon les axes \\(x\\), \\(y\\) et \\(z\\) du repère.\n\\(R_{\\alpha\\beta\\gamma}\\) la matrice de rotation résultat de l\u0026rsquo;application successive (et donc de la multiplication entre elles) des matrices de rotation \\(R_\\gamma\\), \\(R_\\beta\\) et \\(R_\\alpha\\) :\n$$R_\\alpha = \\begin{bmatrix}1 \u0026amp; 0 \u0026amp; 0\\\\0 \u0026amp; \\cos \\alpha \u0026amp; -\\sin \\alpha\\\\0 \u0026amp; \\sin \\alpha \u0026amp; \\cos \\alpha\\end{bmatrix}$$\n$$R_\\beta = \\begin{bmatrix}\\cos \\beta \u0026amp; 0 \u0026amp; -\\sin \\beta\\\\0 \u0026amp; 1 \u0026amp; 0\\\\\\sin \\beta \u0026amp; 0 \u0026amp; \\cos \\beta\\end{bmatrix}$$\n$$R_\\gamma = \\begin{bmatrix}\\cos \\gamma \u0026amp; -\\sin \\gamma \u0026amp; 0\\\\\\sin \\gamma \u0026amp; \\cos \\gamma \u0026amp; 0\\\\0 \u0026amp; 0 \u0026amp; 1\\end{bmatrix}$$\nAvec les bons paramètres extrinsèques \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\), le modèle 3D dans son repère \\(\\mathcal{R_o}\\) pourra être transformé jusqu\u0026rsquo;à être exprimé dans \\(\\mathcal{R_c}\\) puis dans \\(\\mathcal{R_i}\\), en respectant la relation suivante pour chaque point \\(P_o\\) :\n$$ p_i = K_{i \\leftarrow c} M_{c\\leftarrow o} P_o$$\n$$\\begin{bmatrix} u\\\\ v\\\\ 1 \\end{bmatrix}_{\\mathcal{R}_i} = s. \\begin{bmatrix} \\alpha_u \u0026amp; 0 \u0026amp; u_0\\\\ 0 \u0026amp; \\alpha_v \u0026amp; v_0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\bigodot \\begin{bmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; t_x\\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; t_y\\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; t_z\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} . \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ 1 \\end{bmatrix}_{\\mathcal{R}_o} $$\nL\u0026rsquo;opérateur \\(\\bigodot\\) indique la multiplication des termes après avoir enlevé la coordonnée homogène de \\(M_{c\\leftarrow o} P_o\\). Il est simplement là pour que les tailles de matrice soient correctes. Les paramètres intrinsèques sont connus ; pour trouver les paramètres extrinsèques nous utiliserons une méthode de localisation (i.e. d\u0026rsquo;estimation des paramètres) dite par recalages successifs, appelée Perspective-n-Lignes.\nEstimation des paramètres extrinsèques On démarre d\u0026rsquo;une estimation initiale grossière des paramètres \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\). Cette estimation initiale est plus ou moins juste en fonction de la connaissance que l\u0026rsquo;on a a priori de la scène, de l\u0026rsquo;objet, de la position de la caméra.\nDans notre cas, les paramètres initiaux ont pour valeurs \\(alpha = -2.1\\), \\(beta = 0.7\\), \\(gamma = 2.7\\), \\(t_x = 3.1\\), \\(t_y = 1.3\\) et \\(t_z = 18\\), c\u0026rsquo;est-à-dire que l\u0026rsquo;on sait avant même de commencer que le modèle 3D devra subir une rotation d\u0026rsquo;angles \\((-2.1, 0.7, 2.7)\\) autour de ses trois axes, et qu\u0026rsquo;il devra se décaler d\u0026rsquo;environ \\(3cm\\) en \\(x\\), \\(1.5cm\\) en \\(y\\) and \\(18cm\\) en \\(z\\).\nCette connaissance a priori de la scène vient du fait que l\u0026rsquo;on est capable, en tant qu\u0026rsquo;humain, d\u0026rsquo;évaluer la distance de l\u0026rsquo;objet réel par rapport au capteur seulement en regardant l\u0026rsquo;image.\nUtilisation du programme Au lancement du script localisation.py, deux figures s\u0026rsquo;ouvrent : l\u0026rsquo;une représente une scène réelle capturée par la caméra, l\u0026rsquo;autre représente le modèle virtuel à transformer et dont l\u0026rsquo;origine du repère est matérialisée par un point rouge. L\u0026rsquo;objectif étant de venir calquer la boite sur laquelle repose le Pikachu par-dessus le cube bleu ciel posé sur la table, il faut dans un premier temps sélectionner cinq arêtes appartenant à la boite réelle (par click souris sur les extrémités des arêtes), puis sélectionner dans le même ordre les arêtes correspondantes sur le modèle 3D (par click souris au milieu des arêtes). Le nombre de segments à sélectionner (par défaut 5), est fixé dès le départ dans la variable nb_segments.\nDe cette manière, on pourra calculer l\u0026rsquo;erreur commise sur l\u0026rsquo;estimation des paramètres extrinsèques en comparant la distance entre les arêtes sélectionnées dans l\u0026rsquo;image et celles du modèle transformé. Ce critère d\u0026rsquo;erreur est décrit à l\u0026rsquo;étape 2.\nEtape 1 : afficher le modèle dans \\(\\mathcal{R_i}\\) Dans le programme principal, les points du modèle sont stockés dans model3d_Ro, une matrice de taille \\([246\\times6]\\) correspondant aux 246 arêtes du modèle, chacune définie par les 6 coordonnées de ses deux points \\(P_1(x_1, y_1, z_1)\\) et \\(P_2(x_2, y_2, z_2)\\). La matrice de transformation \\(M_{c\\leftarrow o}\\) est stockée dans extrinsic_matrix et les paramètres intrinsèques de la caméra sont stockés dans intrinsic_matrix.\nPour visualiser le modèle 3D dans l\u0026rsquo;image 2D et se faire une idée de la justesse de notre estimation, il faut coder la fonction tranform_and_draw_model qui permet d\u0026rsquo;appliquer la transformation \\(K_{i \\leftarrow c} M_{c\\leftarrow o}\\) à chaque point \\(P_o\\) d\u0026rsquo;un ensemble d\u0026rsquo;arêtes\u0026rsquo; edges_Ro, avec une matrice intrinsic, et une matrice extrinsic pour finalement afficher le résultat dans une figure fig_axis :\ndef transform_and_draw_model(edges_Ro, intrinsic, extrinsic, fig_axis): # ********************************************************************* # # A COMPLETER. # # UTILISER LES FONCTIONS : # # - perspective_projection # # - transform_point_with_matrix # # Input: # # edges_Ro : ndarray[Nx6] # # N = nombre d\u0026#39;aretes dans le modele # # 6 = (X1, Y1, Z1, X2, Y2, Z2) les coordonnees des points # # P1 et P2 de chaque arete # # intrinsic : ndarray[3x3] - parametres intrinseques de la camera # # extrinsic : ndarray[4x4] - parametres extrinseques de la camera # # fig_axis : figure utilisee pour l\u0026#39;affichage # # Output: # # Pas de retour de fonction, mais calcul et affichage des points # # transformes (u1, v1) et (u2, v2) # # ********************************************************************* # # Part to replace # u_1 = np.zeros((edges_Ro.shape[0],1)) u_2 = np.zeros((edges_Ro.shape[0],1)) v_1 = np.zeros((edges_Ro.shape[0],1)) v_2 = np.zeros((edges_Ro.shape[0],1)) ############### for p in range(edges_Ro.shape[0]): fig_axis.plot([u_1[p], u_2[p]], [v_1[p], v_2[p]], \u0026#39;k\u0026#39;) L\u0026rsquo;objectif est de stocker dans les points [u_1, v_1] et [u_2, v_2] les coordonnées \\((u, v)\\) des points \\(P_1\\) et \\(P_2\\) des arêtes après transformation.\nOn peut découper cette fonction en deux sous-étapes :\nla transformation \\(\\mathcal{R_o} \\rightarrow \\mathcal{R_c}\\) des points \\(P_o\\) à l\u0026rsquo;aide de la fonction transform_point_with_matrix fournie dans la librairie matTools :\nP_c = matTools.transform_point_with_matrix(extrinsic, P_o) la projection \\(\\mathcal{R_c} \\rightarrow \\mathcal{R_i}\\) des points nouvellement obtenus \\(P_c\\) à l\u0026rsquo;aide de la fonction perspective_projection qu\u0026rsquo;il faut compléter :\ndef perspective_projection(intrinsic, P_c): # ***************************************************** # # A COMPLETER. # # Fonction utile disponible : # # np.dot # # Input: # # intrinsic : ndarray[3x3] - parametres intrinseques # # P_c : ndarray[Nx3], # # N = nombre de points à transformer # # 3 = (X, Y, Z) les coordonnees des points # # Output: # # u, v : deux ndarray[N] contenant les # # coordonnees Ri des points P_c transformes # # ***************************************************** # u, v = 0, 0 # Part to replace return u, v Une fois perspective_projection et transform_and_draw_model complétées, le lancement du programme global projette le modèle dans l\u0026rsquo;image, avec les paramètres extrinsèques définis au départ.\nLa transformation appliquée à ces points n\u0026rsquo;est visiblement pas très bonne, le modèle ne matche pas la boite comme on le souhaiterait. Pour pouvoir différencier une \u0026ldquo;mauvaise\u0026rdquo; estimation des paramètres extrinsèques d\u0026rsquo;une \u0026ldquo;bonne\u0026rdquo;, et ainsi pouvoir proposer de manière automatique une nouvelle estimation plus proche de notre objectif, il faut déterminer un critère d\u0026rsquo;erreur caractérisant la distance à laquelle on se trouve de cet objectif optimal.\nEtape 2 : déterminer un critère d\u0026rsquo;erreur Le critère d\u0026rsquo;erreur sert à évaluer la justesse de notre estimation. Plus l\u0026rsquo;erreur est grande, moins nos paramètres extrinsèques sont bons. Après avoir déterminé ce critère d\u0026rsquo;erreur, on pourra l\u0026rsquo;intégrer dans une boucle d\u0026rsquo;optimisation visant à le minimiser, et donc à avoir des paramètres extrinsèques les meilleurs possibles pour notre objectif d\u0026rsquo;appariement 2D/3D.\nA titre d\u0026rsquo;exemple, la figure ci-dessous illustre cette boucle d\u0026rsquo;optimisation pour la projection de la boite virtuelle sur le cube réel. A l\u0026rsquo;itération \\(0\\), les paramètres sont grossiers, l\u0026rsquo;erreur est grande. En modifiant les paramètres on fera diminuer l\u0026rsquo;erreur jusqu\u0026rsquo;à atteindre, dans l\u0026rsquo;idéal, une erreur nulle et une transformation optimale pour une projection parfaite du modèle 3D dans l\u0026rsquo;image 2D.\nDans le cadre de l\u0026rsquo;appariement 2D/3D de segments, le critère d\u0026rsquo;erreur à minimiser correspond au produit scalaire de la normale au plan d\u0026rsquo;interprétation relatif à l\u0026rsquo;appariement. En d\u0026rsquo;autres termes, l\u0026rsquo;objectif est de transformer les points du modèle de sorte que les segments sélectionnés dans l\u0026rsquo;image et ceux sélectionnés dans le modèle appartiennent au même plan exprimé dans le repère caméra \\(\\mathcal{R_c}\\).\nComme le montre la figure ci-dessus, on peut définir le plan d\u0026rsquo;interprétation comme étant formé par les segments \\(\\mathcal{l_{i \\rightarrow c}^{j,1}}\\) et \\(\\mathcal{l_{i \\rightarrow c}^{j,2}}\\). Dans cette notation, \\(j\\) correspond au nombre d\u0026rsquo;arêtes sélectionnées au lancement du programme (5 par défaut). Pour chaque arête sélectionnée \\(j\\) et exprimée dans le repère image \\(\\mathcal{R_i}\\), on a donc deux segments \\(l^1\\) et \\(l^2\\). Le segment \\(l^1\\) est composé des deux extrémités \\(P_{i \\rightarrow c}^0\\) et \\(P_{i \\rightarrow c}^1\\) ; le segment \\(l^2\\) est composé des deux extrémités \\(P_{i \\rightarrow c}^1\\) et \\(P_{i \\rightarrow c}^2\\). Chacun des \\(P_{i \\rightarrow c}\\) est un point du repère image \\(\\mathcal{R_i}\\) transformé dans le repère caméra \\(\\mathcal{R_c}\\). Ils sont connus : ce sont ceux qui ont été sélectionnés par click souris.\nUne fois ces segments calculés, on peut calculer la normale au plan d\u0026rsquo;interprétation \\(N_c^j\\). Pour rappel :\n$$N = \\frac{l^1 \\wedge l^2}{||l^1 \\wedge l^2||}$$\nDans la fonction de sélection des segments utils.select_segments(), au fil des sélections d\u0026rsquo;arêtes, les normales sont calculées et stockées dans la matrice normal_vectors grâce à la fonction calculate_normal_vector qu\u0026rsquo;il faut compléter dans le fichier localisation.py :\ndef calculate_normal_vector(p1_Ri, p2_Ri, intrinsic): # ********************************************************* # # A COMPLETER. # # Fonctions utiles disponibles : # # np.dot, np.cross, np.linalg.norm, np.linalg.inv # # Input: # # p1_Ri : list[3] # # 3 = (u, v, 1) du premier point selectionne # # p2_Ri : list[3] # # 3 = (u, v, 1) du deuxieme point selectionne # # intrinsic : ndarray[3x3] des intrinseques # # Output: # # normal_vector : ndarray[3] contenant la normale aux # # segments L1_c et L2_c deduits des # # points image selectionnes # # ********************************************************* # normal_vector = np.zeros((len(p1_Ri),)) # Part to replace return normal_vector Ensuite, pour chaque segment \\(j \\in [1, \u0026hellip;, 5]\\), la distance entre le plan lié aux segments image et les points du modèle 3D peut être calculée grâce au produit scalaire des \\(N_c^j\\) et des \\(P_{o\\rightarrow c}^j\\). Si le produit scalaire est nul, les deux vecteurs sont orthogonaux et le point \\(P_{o\\rightarrow c}^j\\) appartient bien au même plan que \\(P_{i \\rightarrow c}^j\\). Plus le produit scalaire est grand, plus les paramètres extrinsèques s\u0026rsquo;éloignent de la bonne transformation.\nPour résumer, le critère d\u0026rsquo;optimisation des paramètres est \\(\\sum_{j=1}^{2n} F^j(X)^2\\) (le carré enlève les valeurs négatives), avec \\(F^j(X) = N^{j ; \\text{mod} ; 2}.P_{o\\rightarrow c}^{j ; \\text{mod} ; 2, 1|2}\\), selon que l\u0026rsquo;on se trouve sur le point \\(P^{1}\\) ou \\(P^{2}\\).\nDans la somme qui parcourt les points de \\(j\\) à \\(2n\\), \\(j ; \\text{mod} ; 2\\) est l\u0026rsquo;indice du segment pour le point courant. Autrement dit, on cumule les \\((N^i.P^{i, 1})^2\\) et \\((N^i.P^{i, 2})^2\\) pour tous les segments \\(i\\).\nIci, \\(X\\) désigne l\u0026rsquo;ensemble des paramètres \\((\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\) et non une coordonnée. On rappelle que chaque \\(P_{o\\rightarrow c} = \\big[ R_{\\alpha\\beta\\gamma} | T \\big]. P_o\\) ; la valeur de l\u0026rsquo;erreur dépend donc bien de la valeur des paramètres extrinsèques. A chaque passage dans la boucle d\u0026rsquo;optimisation, changer les poids de ces paramètres aura une influence sur le critère \\(F(X)\\).\nLa fonction calculate_error calcule le critère d\u0026rsquo;erreur. Elle prend en entrée le nombre nb_segments d\u0026rsquo;arêtes sélectionnées, les normal_vectors, et les segments_Rc sélectionnés puis transformés par la matrice des paramètres extrinsèques et exprimés dans \\(\\mathcal{R_c}\\).\ndef calculate_error(nb_segments, normal_vectors, segments_Rc): # ***************************************************************** # # A COMPLETER. # # Input: # # nb_segments : par defaut 5 = nombre de segments selectionnes # # normal_vectors : ndarray[Nx3] - normales aux plans # # d\u0026#39;interpretation des segments selectionnes # # N = nombre de segments # # 3 = coordonnees (X,Y,Z) des normales dans Rc # # segments_Rc : ndarray[Nx6] = segments selectionnes dans Ro # # et transformes dans Rc # # N = nombre de segments # # 6 = (X1, Y1, Z1, X2, Y2, Z2) des points P1 et # # P2 des aretes transformees dans Rc # # Output: # # err : float64 - erreur cumulee des distances observe/attendu # # # ***************************************************************** # err = 0 for p in range(nb_segments): # Part to replace with the error calculation err = err + 0 err = np.sqrt(err / 2 * nb_segments) return err D\u0026rsquo;un point de vue mathématique, la somme est écrite pour \\(j\\) allant de \\(1\\) à \\(2n\\), i.e. le nombre de points. D\u0026rsquo;un point de vue implémentation et étant donnée la construction de nos variables, il est plus simple d\u0026rsquo;exprimer le critère au travers d\u0026rsquo;une somme parcourant les arêtes : \\(\\sum_{j=1}^{n} F^j(X)\\) with \\(F^{j}(X) = F^{j, 1}(X)^2 + F^{j, 2}(X)^2\\).\nAinsi, \\(F^{j, 1}(X) = N^j.P_{o\\rightarrow c}^{j, 1}\\) et \\(F^{j, 2}(X) = N^j.P_{o\\rightarrow c}^{j, 2}\\).\nEtape 3 : Estimation des paramètres par la méthode des moindres carrés ordinaires A chaque itération \\(k\\), on cherche un jeu de paramètres \\(X_{k}(\\alpha, \\beta, \\gamma, t_x, t_y, t_z)\\) tel que le critère \\(F(X)\\) soit égal au critère pour le jeu de paramètres précédent \\(X_0\\) (avant mise à jour) incrémenté d\u0026rsquo;un delta pondéré par la jacobienne de la fonction.\nOn cherche donc à résoudre un système de la forme :\n$$F(X) \\approx F(X_0) + J \\Delta X$$\nLa jacobienne \\(J\\) contient sur ses lignes les dérivées partielles de la fonction \\(F\\) pour chaque point sélectionné et selon chacun des paramètres de \\(X\\). Elle traduit la tendance du critère (montant/descendant) et la vitesse à laquelle il augmente ou diminue en fonction de la valeur de chacun de ses paramètres.\nNotre objectif étant d\u0026rsquo;atteindre un critère \\(F(X) = 0\\), on traduit le problème à résoudre par :\n$$ \\begin{align} 0 \u0026amp;= F(X_0) + J \\Delta X \\\\\\ \\Leftrightarrow \\qquad -F(X_0) \u0026amp;= J\\Delta X \\end{align} $$\nL\u0026rsquo;intérêt de travailler par incréments successifs est d\u0026rsquo;avoir des valeurs d\u0026rsquo;incrément si petites par rapport à la dernière itération qu\u0026rsquo;on peut approximer la variation de ces paramètres comme étant égale à 0. Pour rappel, le critère d\u0026rsquo;erreur s\u0026rsquo;exprime de la manière suivante pour chaque segment \\(j\\) :\n$$ \\begin{align} F^{j, 1}(X) \u0026amp;= N^j.P_{o\\rightarrow c}^{j, 1} \\text{ avec } P_{o\\rightarrow c}^{j, 1} = \\big[ R_{\\alpha\\beta\\gamma } | T \\big] . P_o^{j, 1}\\\\\\ F^{j, 2}(X) \u0026amp;= N^{j}.P_{o\\rightarrow c}^{j, 2} \\text{ avec } P_{o\\rightarrow c}^{j, 2} = \\big[ R_{\\alpha\\beta\\gamma } | T \\big] . P_o^{j, 2} \\end{align}$$\nDu fait de toujours avoir des\\(\\Delta X \\approx 0\\), le calcul de la matrice jacobienne se retrouve considérablement simplifié, puisque les dérivées partielles \\((\\frac{\\partial F^j}{\\partial \\alpha}, \\frac{\\partial F^j}{\\partial \\beta}, \\frac{\\partial F^j}{\\partial \\gamma}, \\frac{\\partial F^j}{\\partial t_x}, \\frac{\\partial F^j}{\\partial t_y}, \\frac{\\partial F^j}{\\partial t_z})\\) sont les mêmes à chaque itération.\nLe détail de la dérivation est donné pour le premier paramètre \\(\\alpha\\), les suivants sont à démontrer :\n$$\\begin{align} \\frac{\\partial F^j}{\\partial \\alpha} \u0026amp;= N^j . [R_\\gamma . R_\\beta . \\frac{\\partial R_\\alpha}{\\partial \\alpha} | T] . P_o^j\\\\\\ \\end{align}$$\n\\(R_\\gamma\\) et \\(R_\\beta\\) disparaissent de la dérivation car pour \\(\\gamma \\approx 0\\) et \\(\\beta \\approx 0\\), on a :\n$$ R_{\\gamma \\approx 0} = \\begin{bmatrix} \\cos 0 \u0026amp; -\\sin 0 \u0026amp; 0\\\\\\ \\sin 0 \u0026amp; \\cos 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = I_3 $$\n$$ R_{\\beta \\approx 0} = \\begin{bmatrix} \\cos 0 \u0026amp; 0 \u0026amp; -\\sin 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ \\sin 0 \u0026amp; 0 \u0026amp; \\cos 0 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = I_3 $$\n\\(T\\) disparaît puisque \\(t_x\\), \\(t_y\\), \\(t_z \\approx 0\\).\nRappel de règles de dérivation : \\((\\text{constant } a)\u0026rsquo; \\rightarrow 0 \\text{, } (\\sin)\u0026rsquo; \\rightarrow \\cos \\text{, } (\\cos)\u0026rsquo; \\rightarrow -\\sin\\).\n$$\\begin{align} \\frac{\\partial F^j}{\\partial \\alpha} \u0026amp;= N^j . [I_3 . I_3 . \\frac{\\partial R_\\alpha}{\\partial \\alpha} | 0] . P_o^j\\\\\\ \u0026amp;= N^j . \\frac{\\partial \\begin{bmatrix}1 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; \\cos (\\alpha \\approx 0) \u0026amp; -\\sin (\\alpha \\approx 0)\\\\\\ 0 \u0026amp; \\sin (\\alpha \\approx 0) \u0026amp; \\cos (\\alpha \\approx 0) \\end{bmatrix}}{\\partial \\alpha} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; -\\sin (\\alpha \\approx 0) \u0026amp; -\\cos (\\alpha \\approx 0)\\\\\\ 0 \u0026amp; \\cos (\\alpha \\approx 0) \u0026amp; -\\sin (\\alpha \\approx 0) \\end{bmatrix} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0 \u0026amp; 0 \u0026amp; 0\\\\\\ 0 \u0026amp; 0 \u0026amp; -1\\\\\\ 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} . \\begin{bmatrix}X^j\\\\\\ Y^j\\\\\\ Z^j\\end{bmatrix}_o\\\\\\ \u0026amp;= N^j . \\begin{bmatrix}0\\\\\\ -Z^j\\\\\\ Y^j\\end{bmatrix}_o\\\\\\ \\end{align}$$\nLe raisonnement est le même pour \\(\\frac{\\partial F^j}{\\partial \\beta}, \\frac{\\partial F^j}{\\partial \\gamma}, \\frac{\\partial F^j}{\\partial t_x}, \\frac{\\partial F^j}{\\partial t_y}, \\frac{\\partial F^j}{\\partial t_z}\\) et on obtient :\n$$ \\frac{\\partial F^j}{\\partial \\beta} = N^j . \\begin{bmatrix}Z^j\\\\\\ 0\\\\\\ -X^j\\end{bmatrix}_o \\text{, } \\frac{\\partial F^j}{\\partial \\gamma} = N^j . \\begin{bmatrix}-Y^j\\\\\\ X^j\\\\\\ 0\\end{bmatrix}_o $$ $$ \\frac{\\partial F^j}{\\partial t_x} = N_x^j \\text{, } \\frac{\\partial F^j}{\\partial t_y} = N_y^j \\text{, } \\frac{\\partial F^j}{\\partial t_z} = N_z^j $$\nChacune de ces dérivées partielles est à implémenter dans la fonction partial_derivatives :\ndef partial_derivatives(normal_vector, P_c): # ********************************************************************* # # A COMPLETER. # # Input: # # normal_vector : ndarray[3] contenant la normale pour le segment # # auquel appartient P_c # # P_c : ndarray[3] le point de l\u0026#39;objet transformé dans Rc # # Output: # # partial_derivative : ndarray[6] derivee partielle du critere # # d\u0026#39;erreur pour chacun des parametres extrinseques # # crit_X0 : float64 - valeur du critere pour les parametres # # courants, qui servira de valeur initiale avant # # la mise a jour et le recalcul de l\u0026#39;erreur # # ********************************************************************* # X, Y, Z = P_c[0], P_c[1], P_c[2] partial_derivative = np.zeros((6)) # Variable a remplir partial_derivative[0] = 0 partial_derivative[1] = 0 partial_derivative[2] = 0 partial_derivative[3] = 0 partial_derivative[4] = 0 partial_derivative[5] = 0 # ******************************************************************** crit_X0 = normal_vector[0] * X + normal_vector[1] * Y + normal_vector[2] * Z return partial_derivative, crit_X0 On peut ainsi formaliser le problème comme étant :\n$$ F = J \\Delta X \\text{ avec } F = \\begin{bmatrix}-F^1(X_0) \\\\\\ \\vdots \\\\\\ -F^{2n}(X_0)\\end{bmatrix}_{2n\\times 1} $$ $$ \\text{ et } J = \\begin{bmatrix}\\frac{\\partial F^1}{\\partial \\alpha} \u0026amp; \\frac{\\partial F^1}{\\partial \\beta} \u0026amp; \\frac{\\partial F^1}{\\partial \\gamma} \u0026amp; \\frac{\\partial F^1}{\\partial t_x} \u0026amp; \\frac{\\partial F^1}{\\partial t_y} \u0026amp; \\frac{\\partial F^1}{\\partial t_z}\\\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\\ \\frac{\\partial F^{2n}}{\\partial \\alpha} \u0026amp; \\frac{\\partial F^{2n}}{\\partial \\beta} \u0026amp; \\frac{\\partial F^{2n}}{\\partial \\gamma} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_x} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_y} \u0026amp; \\frac{\\partial F^{2n}}{\\partial t_z}\\end{bmatrix}_{2n\\times 6} $$ $$ \\text{ et } \\Delta X = \\begin{bmatrix}\\Delta \\alpha \\\\\\ \\vdots \\\\\\ \\Delta t_z\\end{bmatrix}_{6 \\times 1} $$\n\\(2n\\) est le nombre de points sélectionnés, \\(n\\) est le nombre d\u0026rsquo;arêtes (une arête = deux extrémités).\n\\(F\\) est connue, \\(J\\) est connue, il ne reste plus qu\u0026rsquo;à estimer \\(\\Delta X\\) de sorte que l\u0026rsquo;égalité \\(F = J \\Delta X\\) soit \u0026ldquo;la plus vraie possible\u0026rdquo;. Pour cela, on souhaite minimiser la distance entre les deux côtés de l\u0026rsquo;égalité :\n$$ \\begin{align} \\min_{\\Delta X} ||F - J\\Delta X||^2 \u0026amp; \\\\\\ \\Leftrightarrow \\qquad \\frac{\\partial ||F - J\\Delta X||^2}{\\partial \\Delta X} \u0026amp;= 0 \\end{align} $$\nEn effet, si la dérivée de la fonction à minimiser est 0, alors la courbe de la fonction a bien atteint un minimum.\nRappel des opérations sur les matrices :\n\\((A+B)^T = A^T + B^T\\)\n\\((AB)^T = B^T A^T\\)\n\\(A\\times B \\neq B\\times A\\)\n\\(A^2 = A^T A\\)\nEn développant \\((F - J \\Delta X)^2\\), on arrive à :\n$$ \\begin{align} (F - J \\Delta X)^2 \u0026amp; = (F - J \\Delta X)^T(F - J \\Delta X)\\\\\\ \u0026amp;= (F^T - \\Delta X^T J^T)(F - J \\Delta X)\\\\\\ \u0026amp;= F^TF - F^TJ\\Delta X - \\Delta X^T J^T F + \\Delta X^T J^T J \\Delta X \\end{align} $$\nOn se sert des propriétés matricielles pour montrer que \\(F^TJ\\Delta X = ((J\\Delta X)^T F)^T = (\\Delta X^T J^T F)^T\\). Considérons les tailles de matrices d\u0026rsquo;un côté et de l\u0026rsquo;autre de l\u0026rsquo;équation : $$ \\begin{align} F^TJ\\Delta X \u0026amp;\\rightarrow [1\\times2n][2n\\times 6][6\\times 1] \\rightarrow [1\\times 1]\\\\\\ (\\Delta X^T J^T F)^T \u0026amp;\\rightarrow [1\\times6][6\\times 2n][2n\\times 1] \\rightarrow [1\\times 1] \\end{align} $$\nEtant donné que chacun de ces termes représente une matrice \\([1\\times 1]\\), on peut écrire \\((\\Delta X^T J^T F)^T = \\Delta X^T J^T F\\), et par conséquent \\(F^TJ\\Delta X = \\Delta X^T J^T F\\).\nOn en déduit : $$ \\begin{align} (F - J \\Delta X)^2 \u0026amp; = F^TF - 2\\Delta X^T J^T F + \\Delta X^T J^T J \\Delta X\\\\\\ \\end{align} $$\nQuelques règles de dérivation :\n\\(\\frac{\\partial AX}{\\partial X} = A^T\\), \\(\\frac{\\partial X^TA^T}{\\partial X} = A^T\\), \\(\\frac{\\partial X^TAX}{\\partial X} = 2AX\\).\n$$ \\begin{align} \\frac{\\partial (F - J \\Delta X)^2}{\\partial \\Delta X} \u0026amp; = -2J^T F + 2 J^T J \\Delta X\\\\\\ \u0026amp;= -J^T F + J^T J \\Delta X\\\\\\ \\Leftrightarrow \\qquad J^T F \u0026amp;= J^T J \\Delta X \\\\\\ (J^TJ)^{-1} J^T F \u0026amp;= \\Delta X \\\\\\ \\end{align} $$\nEn résumé :\nminimiser la distance entre \\(F\\) et \\(J\\Delta X\\) revient à dire que \\(\\frac{\\partial (F - J \\Delta X)^2}{\\partial \\Delta X} = 0\\) la solution est \\(\\Delta X = (J^TJ)^{-1} J^T F\\). La matrice \\(J^+ = (J^TJ)^{-1} J^T\\) s\u0026rsquo;appelle la pseudo-inverse de \\(J\\).\nDans le code Python, on peut ainsi implémenter la mise à jour des paramètres dans la boucle d\u0026rsquo;optimisation. \\(\\Delta X\\) correspond à la variable nommée delta_solution. On peut ensuite passer delta_solution à la fonction matTools.construct_matrix_from_vec qui renvoie une matrice des incréments de \\(X\\) de la même forme que extrinsic (la matrice des paramètres extrinsèques).\nOn incrémente chacun des éléments de extrinsic en la multipliant par delta_extrinsic.\n# ********************************************************************* # # A COMPLETER. # # delta_solution = ... # # delta_extrinsic = matTools.construct_matrix_from_vec(delta_solution) # # extrinsic = ... # # ********************************************************************* # Une fois la boucle d\u0026rsquo;optimisation opérationnelle, on peut visualiser la transformation du modèle sans la boite virtuelle en enlevant les 12 premiers points de model3D_Ro :\nEtape 4 : projection de la pose estimée sur l\u0026rsquo;image prise d\u0026rsquo;un point de vue différent Une deuxième photo a été capturée d\u0026rsquo;un point de vue différent, et la matrice de passage entre le premier et le deuxième point de vue est stockée et chargée en début de programme depuis le fichier .txt correspondant. Elle permet de re-projeter le modèle dans l\u0026rsquo;image issue de la deuxième caméra et d\u0026rsquo;en afficher le résultat :\nfig5 = plt.figure(5) ax5 = fig5.add_subplot(111) ax5.set_xlim(0,720) ax5.set_ylim(480) plt.imshow(image_2) # A completer avec la matrice de passage du repere Ro vers Rc2, avec les matrices # Ro -\u0026gt; Rc et Rc -\u0026gt; Rc2 Ro_to_Rc2 = ... transform_and_draw_model(model3D_Ro[12:], intrinsic_matrix, Ro_to_Rc2, ax5) plt.show(block = False) Bonus pour la fin Grâce à la matrice des paramètres extrinsèques estimée, et tant que l\u0026rsquo;origine du repère objet ne change pas, on peut ajouter des éléments à la scène 3D et les projeter dans l\u0026rsquo;image de manière identique. model3D_Ro_final contient les points d\u0026rsquo;une scène contenant Pikachu et un dinosaure.\nOn décommente les dernières lignes d\u0026rsquo;affichage pour obtenir le résultat final :\nfig6 = plt.figure(6) ax6, lines = utils.plot_3d_model(model3D_Ro_final, fig6) fig7 = plt.figure(7) ax7 = fig7.add_subplot(111) ax7.set_xlim(0, 720) plt.imshow(image) transform_and_draw_model(model3D_Ro_final[12:], intrinsic_matrix, extrinsic_matrix, ax7) plt.show(block=True) TP crée par notre proffesseur Claire Labit-Bonis.\n","date":"July 16, 2022","hero":"/posts/ai/computer-vision/3d_perception/monocular_localization/featured.png","permalink":"https://noegracia.github.io/posts/ai/computer-vision/3d_perception/monocular_localization/","summary":"Iterative estimation of a camera extrinsic parameters.","tags":["Teaching","3D Perception"],"title":"3DP-TP-01 | Localisation monoculaire par PnL itérative"},{"categories":["Basic"],"contents":"Creation of pre-trained autoencoder to learn the initial condensed representation of unlabeled datasets. This architecture consists of 3 parts:\nEncoder: Compresses the input data from the train-validation-test set into a coded representation which is typically smaller by several orders of magnitude than the input data. Latent Space: This space contains the compressed knowledge representations and is thus the most crucial part of the network. Decoder: A module that helps the network to \u0026ldquo;decompress\u0026rdquo; the knowledge representations and reconstruct the data from their coded form. The output is then compared to a ground truth. Imports from time import time import numpy as np import keras.backend as K from keras.layers import Dense, Input, Layer, InputSpec, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv2DTranspose from keras.models import Model from keras.initializers import VarianceScaling from sklearn.cluster import KMeans from sklearn.cluster import MiniBatchKMeans from sklearn import metrics from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt from sklearn.manifold import TSNE from sklearn.decomposition import PCA Loading the data from keras.datasets import mnist from keras.datasets import fashion_mnist import numpy as np # Chargement et normalisation (entre 0 et 1) des données de la base de données MNIST (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = np.reshape(x_train, (len(x_train), 784)) x_test = np.reshape(x_test, (len(x_test), 784)) Classic Autoencoder # Dimension de l\u0026#39;entrée input_img = Input(shape=(784,)) # Dimension de l\u0026#39;espace latent : PARAMETRE A TESTER !! latent_dim = 10 # Définition du encodeur x0 = Dense(500, activation=\u0026#39;relu\u0026#39;)(input_img) x = Dense(200, activation=\u0026#39;relu\u0026#39;)(x0) encoded = Dense(latent_dim, activation=\u0026#39;relu\u0026#39;)(x) # Définition du décodeur decoder_input = Input(shape=(latent_dim,)) x = Dense(200, activation=\u0026#39;relu\u0026#39;)(decoder_input) x1 = Dense(500, activation=\u0026#39;relu\u0026#39;)(x) decoded = Dense(784, activation=\u0026#39;relu\u0026#39;)(x1) # Construction d\u0026#39;un modèle séparé pour pouvoir accéder aux décodeur et encodeur encoder = Model(input_img, encoded) decoder = Model(decoder_input, decoded) # Construction du modèle de l\u0026#39;auto-encodeur encoded = encoder(input_img) decoded = decoder(encoded) autoencoder = Model(input_img, decoded) Summary # Autoencodeur autoencoder.compile(optimizer=\u0026#39;Adam\u0026#39;, loss=\u0026#39;mse\u0026#39;) autoencoder.summary() print(encoder.summary()) print(decoder.summary()) Training autoencoder.fit(x_train, x_train, epochs=20, batch_size=128, shuffle=True, validation_data=(x_test, x_test)) Evaluation # Encode and decode some digits # Note that we take them from the *test* set encoded_imgs = encoder.predict(x_test) decoded_imgs = decoder.predict(encoded_imgs) Visualization n = 10 # How many digits we will display plt.figure(figsize=(20, 4)) for i in range(n): # Display original ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # Display reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() Display # Affichage count=1000 idx = np.random.choice(len(x_test), count) inputs = x_test[idx] coordsAC = encoder.predict(inputs) coordsTSNE = TSNE(n_components=2).fit_transform(inputs.reshape(count, -1)) coordsPCA = PCA(n_components=2).fit_transform(inputs.reshape(count, -1)) classes = y_test[idx] fig, ax = plt.subplots(figsize=(10, 7)) ax.set_title(\u0026#34;Espace latent\u0026#34;) plt.scatter(coordsAC[:, 0], coordsAC[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() fig2, ax2 = plt.subplots(figsize=(10, 7)) ax2.set_title(\u0026#34;ACP sur espace latent\u0026#34;) plt.scatter(coordsPCA[:, 0], coordsPCA[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() fig3, ax3 = plt.subplots(figsize=(10, 7)) ax3.set_title(\u0026#34;tSNE sur espace latent\u0026#34;) plt.scatter(coordsTSNE[:, 0], coordsTSNE[:, 1], c=classes, cmap=\u0026#34;Paired\u0026#34;) plt.colorbar() ","date":"January 10, 2022","hero":"/posts/ai/machine-learning/autoencoder/images/coded-decoded-mnist.jpg","permalink":"https://noegracia.github.io/posts/ai/machine-learning/autoencoder/","summary":"Creation of pre-trained autoencoder to learn the initial condensed representation of unlabeled datasets. This architecture consists of 3 parts:\nEncoder: Compresses the input data from the train-validation-test set into a coded representation which is typically smaller by several orders of magnitude than the input data. Latent Space: This space contains the compressed knowledge representations and is thus the most crucial part of the network. Decoder: A module that helps the network to \u0026ldquo;decompress\u0026rdquo; the knowledge representations and reconstruct the data from their coded form.","tags":["AI","ML","Autoencoder"],"title":"Autoencoder"},{"categories":["Basic"],"contents":"Skin Cancer Detection Tool README.md Overview The objective of this project is to build a Skin Cancer Detection Tool. The tool that we are creating is a segmentation model of spots (moles, melanomas, etc\u0026hellip;) on microscopic images of the skin. To create this tool we will have to train a semantic segmentation AI model. The data that we use for that training is from The International Skin Imaging Collaboration.\nFile Descriptions: data.py: Contains functions to process and load the dataset, preprocess the images, and masks and to create TensorFlow datasets.\nprocess_data(data_path, file_path): Reads the image and mask paths from the dataset. load_data(path): Load training, validation, and test data. read_image(x) and read_mask(x): Read the images and the masks respectively. tf_dataset(x, y, batch=8): Create a TensorFlow dataset. preprocess(x, y): Preprocess the images and masks. predict.py: Uses a pretrained model to make predictions on new images.\nget_data(): Load test images from the INPUT_FOLDER. Then, predictions are made using the loaded model and saved to the OUTPUT_FOLDER. Setup \u0026amp; Requirements Requirements: python 3.x pandas numpy scikit-learn tensorflow 2.x opencv-python You can install these requirements using:\npip install pandas numpy scikit-learn tensorflow opencv-python Steps to Run: Data Preparation:\nPlace your dataset in an appropriate directory. Adjust the paths in the data.py script. Run the data.py script to check if data is loaded properly. python data.py Predicting:\nPlace your test images in the INPUT_FOLDER. Ensure the model path \u0026ldquo;segm_model\u0026rdquo; in predict.py corresponds to your trained model. Run the predict.py script to make predictions. python predict.py Notes This tool currently segments the spots and saves the segmented images in the OUTPUT_FOLDER. You might need to train the model first using your data to get the \u0026ldquo;segm_model\u0026rdquo;. Ensure the directories mentioned in the scripts exist or are modified according to your directory structure. ","date":"August 10, 2021","hero":"/posts/ai/machine-learning/segmentation/images/portada-segm.jpg","permalink":"https://noegracia.github.io/posts/ai/machine-learning/segmentation/","summary":"Skin Cancer Detection Tool README.md Overview The objective of this project is to build a Skin Cancer Detection Tool. The tool that we are creating is a segmentation model of spots (moles, melanomas, etc\u0026hellip;) on microscopic images of the skin. To create this tool we will have to train a semantic segmentation AI model. The data that we use for that training is from The International Skin Imaging Collaboration.\nFile Descriptions: data.","tags":["AI","ML","Autoencoder","Personal Project"],"title":"Skin Cancer Detection using semantic segmentation"},{"categories":null,"contents":"Go Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/go/_index.bn/","summary":"Go Notes ","tags":null,"title":"Go এর নোট সমূহ"},{"categories":null,"contents":"Bash Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://noegracia.github.io/notes/bash/_index.bn/","summary":"Bash Notes ","tags":null,"title":"ব্যাশের নোট সমূহ"}]