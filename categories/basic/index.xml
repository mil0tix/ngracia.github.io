<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Basic on Noé GRACIA</title><link>https://noegracia.github.io/categories/basic/</link><description>Recent content in Basic on Noé GRACIA</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 10 Jan 2022 08:11:25 +0100</lastBuildDate><atom:link href="https://noegracia.github.io/categories/basic/index.xml" rel="self" type="application/rss+xml"/><item><title>Autoencoder</title><link>https://noegracia.github.io/posts/ai/machine-learning/autoencoder/</link><pubDate>Mon, 10 Jan 2022 08:11:25 +0100</pubDate><guid>https://noegracia.github.io/posts/ai/machine-learning/autoencoder/</guid><description>Creation of pre-trained autoencoder to learn the initial condensed representation of unlabeled datasets. This architecture consists of 3 parts:
Encoder: Compresses the input data from the train-validation-test set into a coded representation which is typically smaller by several orders of magnitude than the input data. Latent Space: This space contains the compressed knowledge representations and is thus the most crucial part of the network. Decoder: A module that helps the network to &amp;ldquo;decompress&amp;rdquo; the knowledge representations and reconstruct the data from their coded form.</description></item></channel></rss>